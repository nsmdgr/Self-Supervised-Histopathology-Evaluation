{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Classification Tasks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BACH"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from torchvision.datasets import ImageFolder, MNIST\n",
    "from pathlib import Path\n",
    "from torch.utils.data import random_split, Subset, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torchvision.datasets.utils import download_url, download_and_extract_archive\n",
    "from torch.utils.data import Dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "p = Path('../data')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "mean, std = torch.tensor([0.7169, 0.6170, 0.8427]), torch.tensor([0.1661, 0.1885, 0.1182]) # calculated over dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Bach:\n",
    "    \n",
    "    def __init__(self, root, train_transform, valid_transform, download=False, valid_percent=0.2, shuffle=True):\n",
    "        \n",
    "        self.root = root\n",
    "        self.train_transform = train_transform\n",
    "        self.valid_transform = valid_transform\n",
    "        self.valid_percent = valid_percent\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        if download:\n",
    "            self.download_data()\n",
    "            self.root = self.root/'ICIAR2018_BACH_Challenge/Photos' # extend root directory to point to images\n",
    "            \n",
    "        self.train_ds, self.valid_ds = self.prepare_datasets()\n",
    "        \n",
    "    \n",
    "    def download_data(self):\n",
    "        url = 'https://zenodo.org/record/3632035/files/ICIAR2018_BACH_Challenge.zip'\n",
    "        download_and_extract_archive(url, self.root)\n",
    "    \n",
    "    \n",
    "    def prepare_datasets(self):\n",
    "        train_ds = ImageFolder(self.root, self.train_transform)\n",
    "        valid_ds = ImageFolder(self.root, self.valid_transform)\n",
    "        \n",
    "        num_train = len(train_ds)\n",
    "        indices   = list(range(num_train))\n",
    "        split     = int(np.floor(self.valid_percent * num_train))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            rng = default_rng(seed=101)\n",
    "            rng.shuffle(indices)\n",
    "        \n",
    "        train_idx, valid_idx = indices[split:], indices[:split]\n",
    "        train_ds = Subset(train_ds, train_idx)\n",
    "        valid_ds = Subset(valid_ds, valid_idx)\n",
    "        \n",
    "        return train_ds, valid_ds\n",
    "    \n",
    "    \n",
    "    def get_dataloaders(self, batch_size, shuffle=True, pin_memory=True, num_workers=0, sampler=None):\n",
    "        \n",
    "        train_dl = DataLoader(\n",
    "            self.train_ds, batch_size=batch_size, sampler=sampler, \n",
    "            shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory)\n",
    "        \n",
    "        valid_dl = DataLoader(\n",
    "            self.valid_ds, batch_size=batch_size, sampler=sampler,\n",
    "            num_workers=num_workers, pin_memory=pin_memory)\n",
    "        \n",
    "        return train_dl, valid_dl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "class DatasetFromSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "class Bach:\n",
    "    \n",
    "    def __init__(self, root, train_transform, valid_transform, download=False, valid_percent=0.2, shuffle=True):\n",
    "        \n",
    "        self.root = root\n",
    "        self.train_transform = train_transform\n",
    "        self.valid_transform = valid_transform\n",
    "        self.valid_percent = valid_percent\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        if download:\n",
    "            self.download_data()\n",
    "            self.root = self.root/'ICIAR2018_BACH_Challenge/Photos' # extend root directory to point to images\n",
    "            \n",
    "        self.train_ds, self.valid_ds = self.prepare_datasets()\n",
    "        \n",
    "    \n",
    "    def download_data(self):\n",
    "        url = 'https://zenodo.org/record/3632035/files/ICIAR2018_BACH_Challenge.zip'\n",
    "        download_and_extract_archive(url, self.root)\n",
    "    \n",
    "    \n",
    "    def prepare_datasets(self):\n",
    "        \n",
    "        ds = ImageFolder(self.root)\n",
    "        n_samples = len(ds)\n",
    "\n",
    "        split_lens = [\n",
    "            int(np.floor(n_samples * self.split_pcts[0])), # train\n",
    "            int(np.floor(n_samples * split_pcts[1])), # valid\n",
    "            int(np.floor(n_samples * split_pcts[2]))  # test\n",
    "        ]\n",
    "\n",
    "        train_ds, valid_ds, test_ds = random_split(ds, split_lens, generator=torch.Generator().manual_seed(42))\n",
    "        train_ds = DatasetFromSubset(train_ds, self.train_transform)\n",
    "        valid_ds = DatasetFromSubset(valid_ds, self.valid_transform)\n",
    "        test_ds  = DatasetFromSubset(test_ds,  self.valid_transform)\n",
    "        \n",
    "        return train_ds, valid_ds, test_ds\n",
    "    \n",
    "    \n",
    "    def get_dataloaders(self, batch_size, shuffle=True, pin_memory=True, num_workers=0, sampler=None):\n",
    "        \n",
    "        train_dl = DataLoader(\n",
    "            self.train_ds, batch_size=batch_size, sampler=sampler, \n",
    "            shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory)\n",
    "        \n",
    "        valid_dl = DataLoader(\n",
    "            self.valid_ds, batch_size=batch_size, sampler=sampler,\n",
    "            num_workers=num_workers, pin_memory=pin_memory)\n",
    "        \n",
    "        return train_dl, valid_dl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "train_transform = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor()])\n",
    "\n",
    "valid_transform = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor()])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "root = p / 'ICIAR2018_BACH_Challenge/Photos'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "bach = Bach(root, train_transform, valid_transform, download=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "train_dl, valid_dl = bach.get_dataloaders(32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "x,y = next(iter(train_dl))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "source": [
    "def get_mean_std(loader):\n",
    "    # var[X] = E[X**2] - E[X]**2\n",
    "    channels_sum, channels_sqrd_sum, num_batches = 0, 0, 0\n",
    "\n",
    "    for i, (data, _) in enumerate(loader):\n",
    "        print(f'{i+1}/{len(loader)}', ' '*100, end='\\r')\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_sqrd_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "    std = (channels_sqrd_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "#mean, std = get_mean_std(dl)\n",
    "#print(mean)\n",
    "#print(std)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Patchcamelyon"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "class PatchCamelyonDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, transform, mode='train'):\n",
    "        super().__init__()\n",
    "\n",
    "        assert mode in ['train', 'valid', 'test']\n",
    "        \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "        self.X = h5py.File(root/f'camelyonpatch_level_2_split_{mode}_x.h5', 'r').get('x')\n",
    "        self.y = h5py.File(root/f'camelyonpatch_level_2_split_{mode}_y.h5', 'r').get('y')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.X[idx], self.y[idx]\n",
    "        x, y = self.transform(x), y.item()\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "class PatchCamelyon:\n",
    "    \n",
    "    def __init__(self, root, train_transform, valid_transform, download=False):\n",
    "        \n",
    "        self.root = root\n",
    "        self.train_transform = train_transform\n",
    "        self.valid_transform = valid_transform\n",
    "        \n",
    "        if download:\n",
    "            self.download_data()\n",
    "            \n",
    "        self.train_ds, self.valid_ds, self.test_ds = self.prepare_datasets()\n",
    "        \n",
    "    def download_data(self):\n",
    "        base_url = 'https://zenodo.org/record/2546921/files/'\n",
    "        for mode in ['train', 'valid', 'test']:\n",
    "            download_url(base_url + f'camelyonpatch_level_2_split_{mode}_meta.csv', self.root)\n",
    "            for xy in ['x','y']: \n",
    "                download_and_extract_archive(base_url + f'camelyonpatch_level_2_split_{mode}_{xy}.h5.gz', self.root)\n",
    "    \n",
    "    def prepare_datasets(self):\n",
    "        train_ds = PatchCamelyonDataset(self.root, transform=self.train_transform, mode='train')\n",
    "        valid_ds = PatchCamelyonDataset(self.root, transform=self.valid_transform, mode='valid')\n",
    "        test_ds  = PatchCamelyonDataset(self.root, transform=self.valid_transform, mode='test')\n",
    "        \n",
    "        return train_ds, valid_ds, test_ds\n",
    "    \n",
    "    def get_dataloaders(self, batch_size, shuffle=True, pin_memory=True, num_workers=0):\n",
    "        \n",
    "        train_dl = DataLoader(\n",
    "            self.train_ds, batch_size=batch_size, shuffle=shuffle,\n",
    "            num_workers=num_workers, pin_memory=pin_memory)\n",
    "        \n",
    "        valid_dl = DataLoader(\n",
    "            self.valid_ds, batch_size=batch_size,\n",
    "            num_workers=num_workers, pin_memory=pin_memory)\n",
    "        \n",
    "        test_dl = DataLoader(\n",
    "            self.test_ds, batch_size=batch_size,\n",
    "            num_workers=num_workers, pin_memory=pin_memory)\n",
    "        \n",
    "        return train_dl, valid_dl, test_dl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "tsfm = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "root = Path('/home/niklas/Internal_HDD/project_data/histopathology/pcam/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "pcam = PatchCamelyon(root, train_transform=tsfm, valid_transform=tsfm, download=False)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "train_dl, valid_dl, test_dl = pcam.get_dataloaders(128)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "x,y = next(iter(test_dl))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "x.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 96, 96])"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "mean, std = get_mean_std(train_dl)\n",
    "print(mean)\n",
    "print(std)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.7008, 0.5384, 0.6916])\n",
      "tensor([0.2350, 0.2774, 0.2129])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NCT-CRC-HE-100K"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "root = Path('/home/niklas/Internal_HDD/project_data/histopathology/NCT-CRC-HE-100K/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "class NctCrcHe100K:\n",
    "    \n",
    "    def __init__(self, root, train_transform, valid_transform, download=False, color_norm=True):\n",
    "        \n",
    "        self.root = root\n",
    "        self.train_transform = train_transform\n",
    "        self.valid_transform = valid_transform\n",
    "        self.color_norm = color_norm\n",
    "        \n",
    "        if download:\n",
    "            self.download_data()\n",
    "            \n",
    "        self.train_ds, self.valid_ds = self.prepare_datasets()\n",
    "        \n",
    "    \n",
    "    def download_data(self):\n",
    "        base_url = 'https://zenodo.org/record/1214456/files/'\n",
    "        download_and_extract_archive(base_url + 'NCT-CRC-HE-100K.zip', self.root)\n",
    "        download_and_extract_archive(base_url + 'NCT-CRC-HE-100K-NONORM.zip', self.root)\n",
    "        download_and_extract_archive(base_url + 'CRC-VAL-HE-7K.zip', self.root)\n",
    "    \n",
    "    \n",
    "    def prepare_datasets(self):\n",
    "        train_dir = 'NCT-CRC-HE-100K' if self.color_norm else 'NCT-CRC-HE-100K-NONORM'\n",
    "        \n",
    "        train_ds = ImageFolder(self.root/train_dir, self.train_transform)\n",
    "        valid_ds = ImageFolder(self.root/'CRC-VAL-HE-7K', self.valid_transform)\n",
    "        \n",
    "        return train_ds, valid_ds\n",
    "    \n",
    "    \n",
    "    def get_dataloaders(self, batch_size, shuffle=True, pin_memory=True, num_workers=0):\n",
    "        train_dl = DataLoader(\n",
    "            self.train_ds, batch_size=batch_size, shuffle=shuffle,\n",
    "            num_workers=num_workers, pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        valid_dl = DataLoader(\n",
    "            self.valid_ds, batch_size=batch_size,\n",
    "            num_workers=num_workers, pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        return train_dl, valid_dl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "nct = NctCrcHe100K(root, tsfm, tsfm, download=False, color_norm=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "source": [
    "train_dl, valid_dl = nct.get_dataloaders(128)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "x,y = next(iter(train_dl))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "x.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 224, 224])"
      ]
     },
     "metadata": {},
     "execution_count": 182
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "source": [
    "x,y = next(iter(valid_dl))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "source": [
    "mean, std = get_mean_std(train_dl)\n",
    "print(mean)\n",
    "print(std)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.7358, 0.5804, 0.7012])                                                                            \n",
      "tensor([0.2262, 0.2860, 0.2300])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Breakhis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "source": [
    "class BreakHis:\n",
    "    \n",
    "    def __init__(self, root, train_transform, valid_transform, label='tumor_class', download=False, valid_percent=0.2, shuffle=True):\n",
    "        \n",
    "        self.root = root\n",
    "        self.label = label\n",
    "        self.train_transform = train_transform\n",
    "        self.valid_transform = valid_transform\n",
    "        self.valid_percent = valid_percent\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        assert label in ['tumor_class', 'tumor_type']\n",
    "        \n",
    "        if download:\n",
    "            self.download_data()\n",
    "            \n",
    "        self.ds_train, self.ds_valid, self.ds_test, self.sampler_train, self.sampler_valid, self.sampler_test = self.prepare_datasets()\n",
    "        \n",
    "    def download_data(self):\n",
    "        url = 'http://www.inf.ufpr.br/vri/databases/BreaKHis_v1.tar.gz'\n",
    "        download_and_extract_archive(url, self.root)\n",
    "        self.root = self.root/'BreaKHis_v1/histology_slides/breast' # extend root directory to point to images\n",
    "    \n",
    "    def prepare_datasets(self):\n",
    "        \n",
    "        if self.label == 'tumor_type':\n",
    "            \n",
    "            # multiclass classification\n",
    "            benign_classes    = [0,1,2,3]\n",
    "            malignant_classes = [4,5,6,7]\n",
    "            \n",
    "            benign_types = self.root/'benign/SOB'\n",
    "            malignant_types = self.root/'malignant/SOB'\n",
    "\n",
    "            # instantiate copies of dataset\n",
    "            ds_b_train = ImageFolder(benign_types, self.train_transform)\n",
    "            ds_b_valid = ImageFolder(benign_types, self.valid_transform)\n",
    "            ds_b_test  = ImageFolder(benign_types, self.valid_transform)\n",
    "\n",
    "            ds_m_train = ImageFolder(malignant_types, self.train_transform)\n",
    "            ds_m_valid = ImageFolder(malignant_types, self.valid_transform)\n",
    "            ds_m_test  = ImageFolder(malignant_types, self.valid_transform)\n",
    "            \n",
    "            for ds_m in [ds_m_train, ds_m_valid, ds_m_test]:\n",
    "                # offset classes\n",
    "                img_paths, labels = list(zip(*ds_m.samples))\n",
    "                labels = [label+4 for label in labels]\n",
    "                ds_m_train.targets = labels\n",
    "                ds_m_train.samples = list(zip(img_paths, labels))\n",
    "                \n",
    "            ds_b_train, ds_b_valid, ds_b_test = self._stratified_split(ds_b_train, ds_b_valid, ds_b_test, benign_classes)\n",
    "            ds_m_train, ds_m_valid, ds_m_test = self._stratified_split(ds_m_train, ds_m_valid, ds_m_test, malignant_classes)\n",
    "\n",
    "            ds_train = torch.utils.data.ConcatDataset([ds_b_train, ds_m_train])\n",
    "            ds_valid = torch.utils.data.ConcatDataset([ds_b_valid, ds_m_valid])\n",
    "            ds_test  = torch.utils.data.ConcatDataset([ds_b_test,  ds_m_test])\n",
    "\n",
    "            ds_train.targets = ds_b_train.targets + ds_m_train.targets\n",
    "            ds_valid.targets = ds_b_valid.targets + ds_m_valid.targets\n",
    "            ds_test.targets  = ds_b_test.targets  + ds_m_test.targets\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            classes = [0,1] # binary classification\n",
    "            \n",
    "            ds_train = ImageFolder(self.root, self.train_transform)\n",
    "            ds_valid = ImageFolder(self.root, self.valid_transform)\n",
    "            ds_test  = ImageFolder(self.root, self.valid_transform)\n",
    "            \n",
    "            ds_train, ds_valid, ds_test = self._stratified_split(ds_train, ds_valid, ds_test, classes)\n",
    "            \n",
    "        \n",
    "        sampler_train = self._get_balanced_sampler(ds_train) \n",
    "        sampler_valid = self._get_balanced_sampler(ds_valid)\n",
    "        sampler_test  = self._get_balanced_sampler(ds_test)\n",
    "        \n",
    "        return ds_train, ds_valid, ds_test, sampler_train, sampler_valid, sampler_test\n",
    "    \n",
    "    \n",
    "    def _stratified_split(self, ds_train, ds_valid, ds_test, classes):\n",
    "        \n",
    "        X, y = list(zip(*ds_train.samples))\n",
    "        \n",
    "        stratify = np.repeat(classes, np.ceil(len(X)/len(classes)))[:len(X)]\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y,  stratify=stratify, test_size=0.4) # valid + test = 40% of train\n",
    "\n",
    "        # split valid and test\n",
    "        stratify = np.repeat(classes, np.ceil(len(X_valid)/len(classes)))[:len(X_valid)]\n",
    "        X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid,  stratify=stratify, test_size=0.5) # valid and test are equally sized\n",
    "\n",
    "        # update dataset samples and targets\n",
    "        ds_train.samples = list(zip(X_train, y_train))\n",
    "        ds_train.targets = y_train\n",
    "\n",
    "        ds_valid.samples = list(zip(X_valid, y_valid))\n",
    "        ds_valid.targets = y_valid\n",
    "\n",
    "        ds_test.samples = list(zip(X_test,  y_test))\n",
    "        ds_test.targets = y_test\n",
    "\n",
    "        return ds_train, ds_valid, ds_test \n",
    "    \n",
    "    \n",
    "    def _get_balanced_sampler(self, ds):\n",
    "        \n",
    "        _, class_counts = np.unique(ds.targets, return_counts=True)\n",
    "        n_classes = len(class_counts)\n",
    "        num_samples = len(ds)\n",
    "        labels = copy.copy(ds.targets)\n",
    "\n",
    "        class_weights = [num_samples/class_counts[i] for i in range(n_classes)]\n",
    "        weights = [class_weights[labels[i]] for i in range(num_samples)]\n",
    "        sampler = WeightedRandomSampler(torch.tensor(weights), num_samples)\n",
    "        \n",
    "        return sampler\n",
    "    \n",
    "    \n",
    "    def get_dataloaders(self, batch_size, pin_memory=True, num_workers=0):\n",
    "        train_dl = DataLoader(\n",
    "            self.ds_train, batch_size=batch_size, sampler=self.sampler_train,\n",
    "            num_workers=num_workers, pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        valid_dl = DataLoader(\n",
    "            self.ds_valid, batch_size=batch_size, sampler=self.sampler_valid,\n",
    "            num_workers=num_workers, pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        test_dl = DataLoader(\n",
    "            self.ds_test, batch_size=batch_size, sampler=self.sampler_test,\n",
    "            num_workers=num_workers, pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        return train_dl, valid_dl, test_dl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "source": [
    "root = Path('/home/niklas/Internal_HDD/project_data/histopathology/BreakHis/')\n",
    "url = 'http://www.inf.ufpr.br/vri/databases/BreaKHis_v1.tar.gz'\n",
    "download_and_extract_archive(url, root)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using downloaded and verified file: /home/niklas/Internal_HDD/project_data/histopathology/BreakHis/BreaKHis_v1.tar.gz\n",
      "Extracting /home/niklas/Internal_HDD/project_data/histopathology/BreakHis/BreaKHis_v1.tar.gz to /home/niklas/Internal_HDD/project_data/histopathology/BreakHis\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "source": [
    "p = Path('/home/niklas/Internal_HDD/project_data/histopathology/BreakHis/BreaKHis_v1/histology_slides/breast/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "source": [
    "tsfm = transforms.Compose([transforms.Resize((512, 512)), transforms.ToTensor()])\n",
    "breakhis = BreakHis(p, train_transform=tsfm, valid_transform=tsfm)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "source": [
    "train_dl, valid_dl, test_dl = breakhis.get_dataloaders(batch_size=32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "source": [
    "ds = ImageFolder(p, tsfm)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "source": [
    "dl = DataLoader(ds, batch_size=32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "source": [
    "x,y = next(iter(dl))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "source": [
    "mean, std = get_mean_std(dl)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "248/248                                                                                                     \r"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "source": [
    "mean"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.7871, 0.6265, 0.7644])"
      ]
     },
     "metadata": {},
     "execution_count": 596
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "source": [
    "std"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.1279, 0.1786, 0.1127])"
      ]
     },
     "metadata": {},
     "execution_count": 597
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('pg': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "interpreter": {
   "hash": "c7749a5776c61a0ee3f59932455fac7451f5e8d89b528a1300900c911e68064c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}