{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76745571-ae6f-4952-880c-cbe3ce857fbc",
   "metadata": {},
   "source": [
    "# Classification Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8751f2f-5958-48da-ad32-1ac425dc6219",
   "metadata": {},
   "source": [
    "## BACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d28c03-c4c0-437f-bcc2-1fa5b7bc33f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder, MNIST\n",
    "from pathlib import Path\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torchvision.datasets.utils import download_url, download_and_extract_archive\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "143d0540-282e-459e-8482-2eb31fde18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('/home/niklas/Internal_HDD/project_data/histopathology/BACH/ICIAR2018_BACH_Challenge/Photos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1205160a-9a22-4c6f-8ad2-bbc9a2899030",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = torch.tensor([0.7169, 0.6170, 0.8427]), torch.tensor([0.1661, 0.1885, 0.1182]) # calculated over dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fd19c665-6f92-4200-8b46-731e9716a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bach:\n",
    "    \n",
    "    def __init__(self, root, train_transform, valid_transform, download=False, valid_percent=0.2, shuffle=True):\n",
    "        \n",
    "        self.root = root\n",
    "        self.train_transform = train_transform\n",
    "        self.valid_transform = valid_transform\n",
    "        self.valid_percent = valid_percent\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        if download:\n",
    "            self.download_data()\n",
    "            self.root = self.root/'ICIAR2018_BACH_Challenge/Photos' # extend root directory to point to images\n",
    "            \n",
    "        self.train_ds, self.valid_ds, self.train_sampler, self.valid_sampler = self.prepare_datasets()\n",
    "        \n",
    "    def download_data(self):\n",
    "        url = 'https://zenodo.org/record/3632035/files/ICIAR2018_BACH_Challenge.zip'\n",
    "        download_and_extract_archive(url, self.root)\n",
    "    \n",
    "    def prepare_datasets(self):\n",
    "        train_ds = ImageFolder(self.root, self.train_transform)\n",
    "        valid_ds = ImageFolder(self.root, self.valid_transform)\n",
    "        \n",
    "        num_train = len(train_ds)\n",
    "        indices   = list(range(num_train))\n",
    "        split     = int(np.floor(self.valid_percent * num_train))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            rng = default_rng(seed=101)\n",
    "            rng.shuffle(indices)\n",
    "        \n",
    "        train_idx, valid_idx = indices[split:], indices[:split]\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "        \n",
    "        return train_ds, valid_ds, train_sampler, valid_sampler\n",
    "    \n",
    "    def get_dataloaders(self, batch_size, shuffle=True, pin_memory=True, num_workers=0):\n",
    "        train_dl = DataLoader(\n",
    "            self.train_ds, batch_size=batch_size, sampler=self.train_sampler,\n",
    "            num_workers=num_workers, pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        valid_dl = DataLoader(\n",
    "            self.valid_ds, batch_size=batch_size, sampler=self.valid_sampler,\n",
    "            num_workers=num_workers, pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9c77422a-8912-4e77-868e-f9c407934b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor()])\n",
    "\n",
    "valid_transform = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5f2f950c-4bb2-4943-8e86-817fb5f87d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bach = Bach(p, train_transform, valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9b214fc0-a991-4599-91a9-8171129f8c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = bach.get_dataloaders(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "025f3e51-85b6-466c-b6cd-5c98be9d7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "42b0d064-03b6-4909-8fe0-ff24a0017ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(loader):\n",
    "    # var[X] = E[X**2] - E[X]**2\n",
    "    channels_sum, channels_sqrd_sum, num_batches = 0, 0, 0\n",
    "\n",
    "    for i, (data, _) in enumerate(loader):\n",
    "        print(f'{i+1}/{len(loader)}', ' '*100, end='\\r')\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_sqrd_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "    std = (channels_sqrd_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "#mean, std = get_mean_std(dl)\n",
    "#print(mean)\n",
    "#print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835bece5-7b9a-4a51-a1cb-ab18ba398061",
   "metadata": {},
   "source": [
    "## Patchcamelyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe484867-e822-43ed-8525-89c9df25511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchCamelyonDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, transform, mode='train'):\n",
    "        super().__init__()\n",
    "\n",
    "        assert mode in ['train', 'valid', 'test']\n",
    "        \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "        self.X = h5py.File(root/f'camelyonpatch_level_2_split_{mode}_x.h5', 'r').get('x')\n",
    "        self.y = h5py.File(root/f'camelyonpatch_level_2_split_{mode}_y.h5', 'r').get('y')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.X[idx], self.y[idx]\n",
    "        x, y = self.transform(x), y.item()\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9b36ef15-7a77-4be5-92b5-00bac891bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchCamelyon:\n",
    "    \n",
    "    def __init__(self, root, train_transform, valid_transform, download=False):\n",
    "        \n",
    "        self.root = root\n",
    "        self.train_transform = train_transform\n",
    "        self.valid_transform = valid_transform\n",
    "        \n",
    "        if download:\n",
    "            self.download_data()\n",
    "            \n",
    "        self.train_ds, self.valid_ds, self.test_ds = self.prepare_datasets()\n",
    "        \n",
    "    def download_data(self):\n",
    "        base_url = 'https://zenodo.org/record/2546921/files/'\n",
    "        for mode in ['train', 'valid', 'test']:\n",
    "            download_url(base_url + f'camelyonpatch_level_2_split_{mode}_meta.csv', self.root)\n",
    "            for xy in ['x','y']: \n",
    "                download_and_extract_archive(base_url + f'camelyonpatch_level_2_split_{mode}_{xy}.h5.gz', self.root)\n",
    "    \n",
    "    def prepare_datasets(self):\n",
    "        train_ds = PatchCamelyonDataset(self.root, transform=self.train_transform, mode='train')\n",
    "        valid_ds = PatchCamelyonDataset(self.root, transform=self.valid_transform, mode='valid')\n",
    "        test_ds  = PatchCamelyonDataset(self.root, transform=self.valid_transform, mode='test')\n",
    "        \n",
    "        return train_ds, valid_ds, test_ds\n",
    "    \n",
    "    def get_dataloaders(self, batch_size, shuffle=True, pin_memory=True, num_workers=0):\n",
    "        \n",
    "        train_dl = DataLoader(\n",
    "            self.train_ds, batch_size=batch_size, shuffle=shuffle,\n",
    "            num_workers=num_workers, pin_memory=pin_memory)\n",
    "        \n",
    "        valid_dl = DataLoader(\n",
    "            self.valid_ds, batch_size=batch_size,\n",
    "            num_workers=num_workers, pin_memory=pin_memory)\n",
    "        \n",
    "        test_dl = DataLoader(\n",
    "            self.test_ds, batch_size=batch_size,\n",
    "            num_workers=num_workers, pin_memory=pin_memory)\n",
    "        \n",
    "        return train_dl, valid_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c4cbd4-331d-4bc7-b93b-f0e261f10dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7e271e71-81c0-4690-a3c0-3c5882ce770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfm = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0a4c5f58-8f69-45fa-9af0-4da357276193",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('/home/niklas/Internal_HDD/project_data/histopathology/pcam/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f0413f57-e952-49c9-9d71-edda786aff99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pcam = PatchCamelyon(root, train_transform=tsfm, valid_transform=tsfm, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "41148901-dfc8-4763-a1a5-8b843cc27f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl, test_dl = pcam.get_dataloaders(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "97f735b6-0ba4-4287-aef5-1e0b0f6a4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5c57d369-c06e-4892-89b9-b31ae6c7ceaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 96, 96])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0010932c-b90c-475d-a1ce-10544f3d5d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7008, 0.5384, 0.6916])\n",
      "tensor([0.2350, 0.2774, 0.2129])\n"
     ]
    }
   ],
   "source": [
    "mean, std = get_mean_std(train_dl)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990ebfb-9cea-4090-a349-e1475cba977b",
   "metadata": {},
   "source": [
    "### NCT-CRC-HE-100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "285b1645-a0c9-4380-b1e5-a5c9854bb0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('/home/niklas/Internal_HDD/project_data/histopathology/NCT-CRC-HE-100K/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4af3713e-aac8-4d84-a41b-5deff1dd44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NctCrcHe100K:\n",
    "    \n",
    "    def __init__(self, root, train_transform, valid_transform, download=False, color_norm=True):\n",
    "        \n",
    "        self.root = root\n",
    "        self.train_transform = train_transform\n",
    "        self.valid_transform = valid_transform\n",
    "        self.color_norm = color_norm\n",
    "        \n",
    "        if download:\n",
    "            self.download_data()\n",
    "            \n",
    "        self.train_ds, self.valid_ds = self.prepare_datasets()\n",
    "        \n",
    "    \n",
    "    def download_data(self):\n",
    "        base_url = 'https://zenodo.org/record/1214456/files/'\n",
    "        download_and_extract_archive(base_url + 'NCT-CRC-HE-100K.zip', self.root)\n",
    "        download_and_extract_archive(base_url + 'NCT-CRC-HE-100K-NONORM.zip', self.root)\n",
    "        download_and_extract_archive(base_url + 'CRC-VAL-HE-7K.zip', self.root)\n",
    "    \n",
    "    \n",
    "    def prepare_datasets(self):\n",
    "        train_dir = 'NCT-CRC-HE-100K' if self.color_norm else 'NCT-CRC-HE-100K-NONORM'\n",
    "        \n",
    "        train_ds = ImageFolder(self.root/train_dir, self.train_transform)\n",
    "        valid_ds = ImageFolder(self.root/'CRC-VAL-HE-7K', self.valid_transform)\n",
    "        \n",
    "        return train_ds, valid_ds\n",
    "    \n",
    "    \n",
    "    def get_dataloaders(self, batch_size, shuffle=True, pin_memory=True, num_workers=0):\n",
    "        train_dl = DataLoader(\n",
    "            self.train_ds, batch_size=batch_size, shuffle=shuffle,\n",
    "            num_workers=num_workers, pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        valid_dl = DataLoader(\n",
    "            self.valid_ds, batch_size=batch_size,\n",
    "            num_workers=num_workers, pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "98a38a78-4e25-4cc7-a12c-7301fa4cf0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nct = NctCrcHe100K(root, tsfm, tsfm, download=False, color_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e1abad58-88ef-4837-a360-9b0f8ed15b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = nct.get_dataloaders(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ea2f1fc4-f59f-4d9d-9fa9-85bcd8be4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bf11f3eb-8c93-4abf-96ae-b9598252768a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 224, 224])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d46efd62-2086-4f85-af46-0b7feb5c3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4fbc93fa-398d-44ff-9fc7-495961037fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7358, 0.5804, 0.7012])                                                                            \n",
      "tensor([0.2262, 0.2860, 0.2300])\n"
     ]
    }
   ],
   "source": [
    "mean, std = get_mean_std(train_dl)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50b52b4-5e04-45c3-8524-38f644935ae6",
   "metadata": {},
   "source": [
    "## Breakhis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "61060306-bdc3-45b3-9d3a-e98771c957a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakHis:\n",
    "    \n",
    "    def __init__(self, root, train_transform, valid_transform, label='tumor_class', download=False, valid_percent=0.2, shuffle=True):\n",
    "        \n",
    "        self.root = root\n",
    "        self.label = label\n",
    "        self.train_transform = train_transform\n",
    "        self.valid_transform = valid_transform\n",
    "        self.valid_percent = valid_percent\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        assert label in ['tumor_class', 'tumor_type']\n",
    "        \n",
    "        if download:\n",
    "            self.download_data()\n",
    "            \n",
    "        self.ds_train, self.ds_valid, self.ds_test, self.sampler_train, self.sampler_valid, self.sampler_test = self.prepare_datasets()\n",
    "        \n",
    "    def download_data(self):\n",
    "        url = 'http://www.inf.ufpr.br/vri/databases/BreaKHis_v1.tar.gz'\n",
    "        download_and_extract_archive(url, self.root)\n",
    "        self.root = self.root/'BreaKHis_v1/histology_slides/breast' # extend root directory to point to images\n",
    "    \n",
    "    def prepare_datasets(self):\n",
    "        \n",
    "        if self.label == 'tumor_type':\n",
    "            \n",
    "            # multiclass classification\n",
    "            benign_classes    = [0,1,2,3]\n",
    "            malignant_classes = [4,5,6,7]\n",
    "            \n",
    "            benign_types = self.root/'benign/SOB'\n",
    "            malignant_types = self.root/'malignant/SOB'\n",
    "\n",
    "            # instantiate copies of dataset\n",
    "            ds_b_train = ImageFolder(benign_types, self.train_transform)\n",
    "            ds_b_valid = ImageFolder(benign_types, self.valid_transform)\n",
    "            ds_b_test  = ImageFolder(benign_types, self.valid_transform)\n",
    "\n",
    "            ds_m_train = ImageFolder(malignant_types, self.train_transform)\n",
    "            ds_m_valid = ImageFolder(malignant_types, self.valid_transform)\n",
    "            ds_m_test  = ImageFolder(malignant_types, self.valid_transform)\n",
    "            \n",
    "            for ds_m in [ds_m_train, ds_m_valid, ds_m_test]:\n",
    "                # offset classes\n",
    "                img_paths, labels = list(zip(*ds_m.samples))\n",
    "                labels = [label+4 for label in labels]\n",
    "                ds_m_train.targets = labels\n",
    "                ds_m_train.samples = list(zip(img_paths, labels))\n",
    "                \n",
    "            ds_b_train, ds_b_valid, ds_b_test = self._stratified_split(ds_b_train, ds_b_valid, ds_b_test, benign_classes)\n",
    "            ds_m_train, ds_m_valid, ds_m_test = self._stratified_split(ds_m_train, ds_m_valid, ds_m_test, malignant_classes)\n",
    "\n",
    "            ds_train = torch.utils.data.ConcatDataset([ds_b_train, ds_m_train])\n",
    "            ds_valid = torch.utils.data.ConcatDataset([ds_b_valid, ds_m_valid])\n",
    "            ds_test  = torch.utils.data.ConcatDataset([ds_b_test,  ds_m_test])\n",
    "\n",
    "            ds_train.targets = ds_b_train.targets + ds_m_train.targets\n",
    "            ds_valid.targets = ds_b_valid.targets + ds_m_valid.targets\n",
    "            ds_test.targets  = ds_b_test.targets  + ds_m_test.targets\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            classes = [0,1] # binary classification\n",
    "            \n",
    "            ds_train = ImageFolder(self.root, self.train_transform)\n",
    "            ds_valid = ImageFolder(self.root, self.valid_transform)\n",
    "            ds_test  = ImageFolder(self.root, self.valid_transform)\n",
    "            \n",
    "            ds_train, ds_valid, ds_test = self._stratified_split(ds_train, ds_valid, ds_test, classes)\n",
    "            \n",
    "        \n",
    "        sampler_train = self._get_balanced_sampler(ds_train) \n",
    "        sampler_valid = self._get_balanced_sampler(ds_valid)\n",
    "        sampler_test  = self._get_balanced_sampler(ds_test)\n",
    "        \n",
    "        return ds_train, ds_valid, ds_test, sampler_train, sampler_valid, sampler_test\n",
    "    \n",
    "    \n",
    "    def _stratified_split(self, ds_train, ds_valid, ds_test, classes):\n",
    "        \n",
    "        X, y = list(zip(*ds_train.samples))\n",
    "        \n",
    "        stratify = np.repeat(classes, np.ceil(len(X)/len(classes)))[:len(X)]\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y,  stratify=stratify, test_size=0.4) # valid + test = 40% of train\n",
    "\n",
    "        # split valid and test\n",
    "        stratify = np.repeat(classes, np.ceil(len(X_valid)/len(classes)))[:len(X_valid)]\n",
    "        X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid,  stratify=stratify, test_size=0.5) # valid and test are equally sized\n",
    "\n",
    "        # update dataset samples and targets\n",
    "        ds_train.samples = list(zip(X_train, y_train))\n",
    "        ds_train.targets = y_train\n",
    "\n",
    "        ds_valid.samples = list(zip(X_valid, y_valid))\n",
    "        ds_valid.targets = y_valid\n",
    "\n",
    "        ds_test.samples = list(zip(X_test,  y_test))\n",
    "        ds_test.targets = y_test\n",
    "\n",
    "        return ds_train, ds_valid, ds_test \n",
    "    \n",
    "    \n",
    "    def _get_balanced_sampler(self, ds):\n",
    "        \n",
    "        _, class_counts = np.unique(ds.targets, return_counts=True)\n",
    "        n_classes = len(class_counts)\n",
    "        num_samples = len(ds)\n",
    "        labels = copy.copy(ds.targets)\n",
    "\n",
    "        class_weights = [num_samples/class_counts[i] for i in range(n_classes)]\n",
    "        weights = [class_weights[labels[i]] for i in range(num_samples)]\n",
    "        sampler = WeightedRandomSampler(torch.tensor(weights), num_samples)\n",
    "        \n",
    "        return sampler\n",
    "    \n",
    "    \n",
    "    def get_dataloaders(self, batch_size, pin_memory=True, num_workers=0):\n",
    "        train_dl = DataLoader(\n",
    "            self.ds_train, batch_size=batch_size, sampler=self.sampler_train,\n",
    "            num_workers=num_workers, pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        valid_dl = DataLoader(\n",
    "            self.ds_valid, batch_size=batch_size, sampler=self.sampler_valid,\n",
    "            num_workers=num_workers, pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        test_dl = DataLoader(\n",
    "            self.ds_test, batch_size=batch_size, sampler=self.sampler_test,\n",
    "            num_workers=num_workers, pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        return train_dl, valid_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "d9320d67-e7bc-4e15-802c-4f0ee5b84c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/niklas/Internal_HDD/project_data/histopathology/BreakHis/BreaKHis_v1.tar.gz\n",
      "Extracting /home/niklas/Internal_HDD/project_data/histopathology/BreakHis/BreaKHis_v1.tar.gz to /home/niklas/Internal_HDD/project_data/histopathology/BreakHis\n"
     ]
    }
   ],
   "source": [
    "root = Path('/home/niklas/Internal_HDD/project_data/histopathology/BreakHis/')\n",
    "url = 'http://www.inf.ufpr.br/vri/databases/BreaKHis_v1.tar.gz'\n",
    "download_and_extract_archive(url, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "4c5f87fd-df17-4bd1-8431-7f8278ef4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('/home/niklas/Internal_HDD/project_data/histopathology/BreakHis/BreaKHis_v1/histology_slides/breast/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "68b23b38-4023-4087-938b-9d04a70a444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfm = transforms.Compose([transforms.Resize((512, 512)), transforms.ToTensor()])\n",
    "breakhis = BreakHis(p, train_transform=tsfm, valid_transform=tsfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "ba88559e-a042-4af6-a8d5-56bf774a8c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl, test_dl = breakhis.get_dataloaders(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "30cc33b6-dd97-4c3c-bd1d-89d63be817e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ImageFolder(p, tsfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "59818020-da0d-44fd-a15f-1b35c424d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "b952dd03-94bb-4f34-9daf-b81eb71daf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "baeaa67f-0239-42b6-a114-001aecbf0249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "mean, std = get_mean_std(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "af4e4311-4610-4414-9d0d-d63c0c1f2639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7871, 0.6265, 0.7644])"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "12ad9e0c-1aff-4503-85e7-5c2a93667f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1279, 0.1786, 0.1127])"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742a1da-8aff-42d2-b4b4-23772466a9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
